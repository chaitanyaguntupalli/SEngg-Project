{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3a7cbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acd0a1f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RepoLang</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jupyter Notebook</td>\n",
       "      <td>ONNX model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TypeScript</td>\n",
       "      <td>An utility method to trigger a function when s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>屠呦呦：跨界者还是守界者？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Go</td>\n",
       "      <td>documentation on reverting / untracking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TypeScript</td>\n",
       "      <td>랜덤한 id 생성하기</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>C++</td>\n",
       "      <td>how to install on proot ubuntu of termux</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>TypeScript</td>\n",
       "      <td>v3 to v4 Migration Guide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>JavaScript</td>\n",
       "      <td>2023-06-9 - Databases, data warehouses, and da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>HTML</td>\n",
       "      <td>AI &amp; CLI Assignment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Python</td>\n",
       "      <td>Exploring the Plotting of Pair Correlation Fun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            RepoLang                                              Title\n",
       "0   Jupyter Notebook                                         ONNX model\n",
       "1         TypeScript  An utility method to trigger a function when s...\n",
       "2               None                                      屠呦呦：跨界者还是守界者？\n",
       "3                 Go            documentation on reverting / untracking\n",
       "4         TypeScript                                        랜덤한 id 생성하기\n",
       "..               ...                                                ...\n",
       "56               C++           how to install on proot ubuntu of termux\n",
       "57        TypeScript                           v3 to v4 Migration Guide\n",
       "58        JavaScript  2023-06-9 - Databases, data warehouses, and da...\n",
       "59              HTML                                AI & CLI Assignment\n",
       "60            Python  Exploring the Plotting of Pair Correlation Fun...\n",
       "\n",
       "[61 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = '20231012_235320_discussion_sharings.json'\n",
    "data = pd.read_json(file_path)\n",
    "data = pd.json_normalize(data['Sources'])\n",
    "repolang = list(data[\"RepoLanguage\"])\n",
    "title = list(data[\"Title\"])\n",
    "df = pd.DataFrame({'RepoLang': repolang, 'Title': title})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede25e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '20231012_235320_discussion_sharings.json'\n",
    "data = pd.read_json(file_path)\n",
    "data = pd.json_normalize(data['Sources'])\n",
    "chatgpt_sharing_data = pd.json_normalize(data['ChatgptSharing'].explode())\n",
    "first_column_data = chatgpt_sharing_data.iloc[:, 0]\n",
    "first_column_df = pd.DataFrame(first_column_data)\n",
    "expanded_chatgpt_sharing = data['ChatgptSharing'].apply(pd.Series).stack().reset_index(drop=True)\n",
    "all_attributes_chatgpt_sharing = pd.json_normalize(expanded_chatgpt_sharing)\n",
    "discussions_conversations_data = pd.json_normalize(all_attributes_chatgpt_sharing['Conversations'].explode())\n",
    "discussions_conversations_data[\"type\"] = \"Discussions\"\n",
    "discussions_conversations_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab4a69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '20231012_233628_pr_sharings.json'\n",
    "data = pd.read_json(file_path)\n",
    "data = pd.json_normalize(data['Sources'])\n",
    "chatgpt_sharing_data = pd.json_normalize(data['ChatgptSharing'].explode())\n",
    "first_column_data = chatgpt_sharing_data.iloc[:, 0]\n",
    "first_column_df = pd.DataFrame(first_column_data)\n",
    "expanded_chatgpt_sharing = data['ChatgptSharing'].apply(pd.Series).stack().reset_index(drop=True)\n",
    "all_attributes_chatgpt_sharing = pd.json_normalize(expanded_chatgpt_sharing)\n",
    "pr_conversations_data = pd.json_normalize(all_attributes_chatgpt_sharing['Conversations'].explode())\n",
    "pr_conversations_data[\"type\"] = \"PR\"\n",
    "pr_conversations_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d73942",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '20231012_235128_issue_sharings.json'\n",
    "data = pd.read_json(file_path)\n",
    "data = pd.json_normalize(data['Sources'])\n",
    "chatgpt_sharing_data = pd.json_normalize(data['ChatgptSharing'].explode())\n",
    "first_column_data = chatgpt_sharing_data.iloc[:, 0]\n",
    "first_column_df = pd.DataFrame(first_column_data)\n",
    "expanded_chatgpt_sharing = data['ChatgptSharing'].apply(pd.Series).stack().reset_index(drop=True)\n",
    "all_attributes_chatgpt_sharing = pd.json_normalize(expanded_chatgpt_sharing)\n",
    "issue_conversations_data = pd.json_normalize(all_attributes_chatgpt_sharing['Conversations'].explode())\n",
    "issue_conversations_data[\"type\"] = \"Issue\"\n",
    "issue_conversations_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10cd0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '20231012_230826_commit_sharings.json'\n",
    "data = pd.read_json(file_path)\n",
    "data = pd.json_normalize(data['Sources'])\n",
    "chatgpt_sharing_data = pd.json_normalize(data['ChatgptSharing'].explode())\n",
    "first_column_data = chatgpt_sharing_data.iloc[:, 0]\n",
    "first_column_df = pd.DataFrame(first_column_data)\n",
    "expanded_chatgpt_sharing = data['ChatgptSharing'].apply(pd.Series).stack().reset_index(drop=True)\n",
    "all_attributes_chatgpt_sharing = pd.json_normalize(expanded_chatgpt_sharing)\n",
    "commit_conversations_data = pd.json_normalize(all_attributes_chatgpt_sharing['Conversations'].explode())\n",
    "commit_conversations_data[\"type\"] = \"Commit\"\n",
    "commit_conversations_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9af088",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '20231012_234250_file_sharings.json'\n",
    "data = pd.read_json(file_path)\n",
    "data = pd.json_normalize(data['Sources'])\n",
    "chatgpt_sharing_data = pd.json_normalize(data['ChatgptSharing'].explode())\n",
    "first_column_data = chatgpt_sharing_data.iloc[:, 0]\n",
    "first_column_df = pd.DataFrame(first_column_data)\n",
    "expanded_chatgpt_sharing = data['ChatgptSharing'].apply(pd.Series).stack().reset_index(drop=True)\n",
    "all_attributes_chatgpt_sharing = pd.json_normalize(expanded_chatgpt_sharing)\n",
    "file_conversations_data = pd.json_normalize(all_attributes_chatgpt_sharing['Conversations'].explode())\n",
    "file_conversations_data[\"type\"] = \"File\"\n",
    "file_conversations_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63496337",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '20231012_232232_hn_sharings.json'\n",
    "data = pd.read_json(file_path)\n",
    "data = pd.json_normalize(data['Sources'])\n",
    "chatgpt_sharing_data = pd.json_normalize(data['ChatgptSharing'].explode())\n",
    "first_column_data = chatgpt_sharing_data.iloc[:, 0]\n",
    "first_column_df = pd.DataFrame(first_column_data)\n",
    "expanded_chatgpt_sharing = data['ChatgptSharing'].apply(pd.Series).stack().reset_index(drop=True)\n",
    "all_attributes_chatgpt_sharing = pd.json_normalize(expanded_chatgpt_sharing)\n",
    "hn_conversations_data = pd.json_normalize(all_attributes_chatgpt_sharing['Conversations'].explode())\n",
    "hn_conversations_data[\"type\"] = \"HN\"\n",
    "hn_conversations_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3e2cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([discussions_conversations_data, pr_conversations_data, issue_conversations_data, commit_conversations_data, file_conversations_data, hn_conversations_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcd3e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2e4d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(data[\"type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8feb717",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270c03ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3610f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_values = data[data.isna().any(axis = 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df88f61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nan_rows = data[data.isna().all(axis = 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7b139e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nan_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4f1ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_nan_rows = data[data.isna().sum(axis=1) == 1]\n",
    "one_nan_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06b776a",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_nan_rows = data[data.isna().sum(axis=1) == 2]\n",
    "two_nan_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ba21c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "three_nan_rows = data[data.isna().sum(axis=1) == 3]\n",
    "three_nan_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc13324",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data.copy() #gonna preserve the original databframe\n",
    "data1.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d394f587",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55825b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db278933",
   "metadata": {},
   "source": [
    "### **LATER, find out what columns have [] in the list of code column - these prompts gave the no-code solution to prompts!!!!**\n",
    "\n",
    "## 11/13/2023 Documentation #1\n",
    "\n",
    "### Data Collection and Transformation\n",
    "1. The snapshot chosen from the given list of snapshots is **snapshot_20230831**.\n",
    "2. There are 6 different json files in this snapshot. They are - PR sharings, Issue Sharings, Discussion Sharings, Commit Sharings, File Sharings and HN Sharings.\n",
    "3. The columns - Prompt, Answer, List of Code is extracted from ChatgptSharing column of every json file. An extra column called Type which specifed which of the 6 different types the rows come under is created.\n",
    "4. All the 6 json files (extracted and shrunk down) are merged together.\n",
    "\n",
    "### Data Cleaning\n",
    "1. Null values are identified in the merged file.\n",
    "2. These null values are then dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd9c668",
   "metadata": {},
   "outputs": [],
   "source": [
    "num27353 =  data1[\"Prompt\"][27353]\n",
    "num27353"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1d153c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "import string\n",
    "import networkx as nx\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def read_and_preprocess_text(text):\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "    # Tokenize each sentence into words, remove stopwords and punctuation, and convert to lowercase\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english') + list(string.punctuation))\n",
    "    tokenized_sentences = [\n",
    "        [word.lower() for word in nltk.word_tokenize(sentence) if word.isalnum() and word.lower() not in stop_words]\n",
    "        for sentence in sentences\n",
    "    ]\n",
    "\n",
    "    return tokenized_sentences\n",
    "\n",
    "def calculate_cosine_similarity(tfidf_matrix):\n",
    "    # Calculate cosine similarity between sentences based on TF-IDF matrix\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "    return similarity_matrix\n",
    "\n",
    "def generate_summary(text, num_sentences=3):\n",
    "    # Preprocess the text\n",
    "    tokenized_sentences = read_and_preprocess_text(text)\n",
    "\n",
    "    # Convert sentences to TF-IDF matrix\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([' '.join(sentence) for sentence in tokenized_sentences])\n",
    "\n",
    "    # Calculate cosine similarity matrix\n",
    "    similarity_matrix = calculate_cosine_similarity(tfidf_matrix)\n",
    "\n",
    "    # Use the PageRank algorithm to rank sentences\n",
    "    graph = nx.from_numpy_array(similarity_matrix)\n",
    "    scores = nx.pagerank(graph)\n",
    "\n",
    "    # Sort sentences by their scores and select the top ones\n",
    "    ranked_sentences = sorted(((scores[i], sentence) for i, sentence in enumerate(tokenized_sentences)), reverse=True)\n",
    "    summary_sentences = [sentence for score, sentence in ranked_sentences[:num_sentences]]\n",
    "\n",
    "    # Join the summary sentences to form the final summary\n",
    "    summary = ' '.join([' '.join(sentence) for sentence in summary_sentences])\n",
    "\n",
    "    return summary\n",
    "\n",
    "# Example usage\n",
    "text = 'I have a vue 3 application. I have a ref constant which is a list. When nothing changed to the ref for 3 seconds, I want to trigger a method. What do I need?'\n",
    "\n",
    "summary = generate_summary(text, num_sentences=3)\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96d87de",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_summary(num27353, num_sentences=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094adb36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
